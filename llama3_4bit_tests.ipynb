{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q accelerate bitsandbytes transformers torch datasets evaluate tabulate fsspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch, time\n",
    "from config import ModelArgs, HF_MODEL_PATH, HF_TOKENIZER_PATH\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ModelArgs()\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using fp4 quant (bitsandbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model in fp4 using bitsandbytes\n",
    "fp4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"fp4\",\n",
    ")\n",
    "fp4_model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32,\n",
    "    quantization_config=fp4_config\n",
    ")\n",
    "print(fp4_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using nf4 quant bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model in nf4 using bitsandbytes\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "nf4_model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32,\n",
    "    quantization_config=nf4_config\n",
    ")\n",
    "print(nf4_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_TOKENIZER_PATH, trust_remote_code=True)\n",
    "\n",
    "\n",
    "prompt = \"Today was a perfect day\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with torch.float32\n",
      "Today was a perfect day for a walk. The weather was sunny and warm, the air was crisp and clean, and the sky was a deep blue. I was in a good mood, and I was in the mood to walk. I had a long list of things to do, but I didnâ€™t want to do them all at once. I wanted to take my time and enjoy the walk.\n",
      "I started out by walking down the street. I walked for a few minutes, and then I turned around and walked back. I walked for a few more minutes, and then I turned around again and walked back. I walked for a few more minutes, and then I turned around again and walked back. I walked for a few more minutes, and then I turned around again\n",
      "\n",
      "Token count: 150, elapsed: 1.38s, 109 tokens/s\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "start = time.time()\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=args.max_new_tokens,\n",
    "    do_sample=args.do_sample,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "tokens_generated = output_ids.shape[-1] - inputs.input_ids.shape[-1]\n",
    "print(f\"with torch.float32\")\n",
    "fp32_out = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(fp32_out)\n",
    "print()\n",
    "print(f\"Token count: {tokens_generated}, elapsed: {elapsed:.2f}s, {tokens_generated/elapsed:.0f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with fp4\n",
      "Today was a perfect day to go to the beach. The water was calm and the waves were small. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet. I was able to walk on the beach and not get my feet wet\n",
      "Token count: 150, elapsed: 0.94s, 160 tokens/s\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(fp4_model.device)\n",
    "start = time.time()\n",
    "output_ids = fp4_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=args.max_new_tokens,\n",
    "    do_sample=args.do_sample,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "tokens_generated = output_ids.shape[-1] - inputs.input_ids.shape[-1]\n",
    "print(f\"with fp4\")\n",
    "fp4_out = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(fp4_out)\n",
    "print(f\"Token count: {tokens_generated}, elapsed: {elapsed:.2f}s, {tokens_generated/elapsed:.0f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with nf4\n",
      "Today was a perfect day to go to the beach. We went to the beach in the morning and then went to the park in the afternoon. We had a great time. We went to the park and played on the swings and the slides. We also went to the playground and played on the jungle gym. We had a great time. We also went to the beach and played on the swings and the slides. We also went to the playground and played on the jungle gym. We had a great time. We also went to the beach and played on the swings and the slides. We also went to the playground and played on the jungle gym. We had a great time. We also went to the beach and played on the swings and the slides. We also went to\n",
      "Token count: 150, elapsed: 0.93s, 162 tokens/s\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(nf4_model.device)\n",
    "start = time.time()\n",
    "output_ids = nf4_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=args.max_new_tokens,\n",
    "    do_sample=args.do_sample,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "tokens_generated = output_ids.shape[-1] - inputs.input_ids.shape[-1]\n",
    "print(f\"with nf4\")\n",
    "nf4_out = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(nf4_out)\n",
    "print(f\"Token count: {tokens_generated}, elapsed: {elapsed:.2f}s, {tokens_generated/elapsed:.0f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from math import exp\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts, max_length=512):\n",
    "    model.eval()\n",
    "    ppl_scores = []\n",
    "    for text in texts:\n",
    "        enc = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "        input_ids = enc.input_ids.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        ppl_scores.append(exp(loss.item()))\n",
    "    return ppl_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 PPL: [14.894348644381546, 47.592817913932805, 57.660402623079534]\n",
      "FP4  PPL: [15.627858879456415, 35.94554888214959, 62.58180678166403]\n",
      "NF4  PPL: [13.277682313925535, 54.149774907452674, 60.16393450325215]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The capital of France is\",\n",
    "    \"What is 17 * 28?\"\n",
    "]\n",
    "\n",
    "\n",
    "ppl_fp32 = compute_perplexity(model, tokenizer, texts)\n",
    "ppl_fp4 = compute_perplexity(fp4_model, tokenizer, texts)\n",
    "ppl_nf4 = compute_perplexity(nf4_model, tokenizer, texts)\n",
    "print(\"FP32 PPL:\", ppl_fp32)\n",
    "print(\"FP4  PPL:\", ppl_fp4)\n",
    "print(\"NF4  PPL:\", ppl_nf4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text                        FP32 PPL    FP4 PPL    NF4 PPL\n",
      "------------------------  ----------  ---------  ---------\n",
      "Once upon a time               14.89      15.63      13.28\n",
      "The capital of France is       47.59      35.95      54.15\n",
      "What is 17 * 28?               57.66      62.58      60.16\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "rows = []\n",
    "for i, text in enumerate(texts):\n",
    "    rows.append([\n",
    "        text,\n",
    "        round(ppl_fp32[i], 2),\n",
    "        round(ppl_fp4[i], 2),\n",
    "        round(ppl_nf4[i], 2)\n",
    "    ])\n",
    "\n",
    "print(tabulate(rows, headers=[\"Text\", \"FP32 PPL\", \"FP4 PPL\", \"NF4 PPL\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cais/mmlu\", \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mmlu_prompt(question, choices):\n",
    "    formatted_choices = \"\\n\".join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
    "    return f\"Question: {question}\\n\\nChoices:\\n{formatted_choices}\\n\\nAnswer:\"\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Take a subset if num_samples is specified\n",
    "    samples = dataset if num_samples is None else dataset.select(range(num_samples))\n",
    "    \n",
    "    results = []\n",
    "    for item in samples:\n",
    "        # Format the prompt\n",
    "        prompt = format_mmlu_prompt(item['question'], item['choices'])\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Get the predicted answer\n",
    "        generated = tokenizer.decode(outputs[0][-1:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Map the output to A, B, C, D\n",
    "        pred_idx = None\n",
    "        for i, letter in enumerate(['A', 'B', 'C', 'D']):\n",
    "            if letter in generated:\n",
    "                pred_idx = i\n",
    "                break\n",
    "        \n",
    "        # If no valid letter found, count as wrong\n",
    "        if pred_idx is None:\n",
    "            correct_answer = chr(65 + item['answer'])\n",
    "            results.append({\n",
    "                'question': item['question'],\n",
    "                'prediction': generated,\n",
    "                'correct': correct_answer,\n",
    "                'is_correct': False\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        # Check if correct\n",
    "        is_correct = pred_idx == item['answer']\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        results.append({\n",
    "            'question': item['question'],\n",
    "            'prediction': chr(65 + pred_idx),\n",
    "            'correct': chr(65 + item['answer']),\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMLU Abstract Algebra dataset...\n",
      "\n",
      "Evaluating FP32 model...\n",
      "\n",
      "Evaluating FP4 model...\n",
      "\n",
      "Evaluating NF4 model...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading MMLU Abstract Algebra dataset...\")\n",
    "eval_ds = ds[\"test\"]  # Using test split for evaluation\n",
    "\n",
    "# Number of samples to evaluate\n",
    "num_eval_samples = 1000\n",
    "\n",
    "print(\"\\nEvaluating FP32 model...\")\n",
    "fp32_acc, fp32_results = evaluate_model(model, tokenizer, eval_ds, num_eval_samples)\n",
    "\n",
    "print(\"\\nEvaluating FP4 model...\")\n",
    "fp4_acc, fp4_results = evaluate_model(fp4_model, tokenizer, eval_ds, num_eval_samples)\n",
    "\n",
    "print(\"\\nEvaluating NF4 model...\")\n",
    "nf4_acc, nf4_results = evaluate_model(nf4_model, tokenizer, eval_ds, num_eval_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results(% of 1000):\n",
      "FP32 Accuracy: 33.90%\n",
      "FP4 Accuracy:  31.50%\n",
      "NF4 Accuracy:  36.30%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"\\nResults(% of {num_eval_samples}):\")\n",
    "print(f\"FP32 Accuracy: {fp32_acc:.2%}\")\n",
    "print(f\"FP4 Accuracy:  {fp4_acc:.2%}\")\n",
    "print(f\"NF4 Accuracy:  {nf4_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Results:\n",
      "\n",
      "FP32 Model:\n",
      "Q: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "Predicted: B, Correct: B, Is Correct: True\n",
      "\n",
      "Q: Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the index of <p> in S_5.\n",
      "Predicted: B, Correct: C, Is Correct: False\n",
      "\n",
      "Q: Find all zeros in the indicated finite field of the given polynomial with coefficients in that field. x^5 + 3x^3 + x^2 + 2x in Z_5\n",
      "Predicted: A, Correct: D, Is Correct: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print detailed results for first few examples\n",
    "print(\"\\nSample Results:\")\n",
    "print(\"\\nFP32 Model:\")\n",
    "for result in fp32_results[:3]:\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"Predicted: {result['prediction']}, Correct: {result['correct']}, Is Correct: {result['is_correct']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
