(llama3.npy) (base) swap357@Mac llama3.npy % python llama3.py "Once upon a time"                                          
INFO:root:Loading tokenizer from /Users/swap357/Documents/dev/llama3.npy/tokenizer.model.np
INFO:root:Tokenizer initialized with 128256 tokens
INFO:root:BOS ID: 128000, EOS ID: 128001
INFO:root:Special tokens: {'<|begin_of_text|>', '<|end_of_text|>'}
INFO:root:Has space token: True
INFO:utils:Loading model from /Users/swap357/Documents/dev/llama3.npy/model.npz
INFO:utils:model.embed_tokens.weight: shape=(128256, 2048), dtype=float32, min=-0.3027, max=0.3457, mean=-0.0001, std=0.0219
INFO:utils:model.layers.0.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.7070, max=0.5977, mean=0.0000, std=0.0369
INFO:utils:model.layers.0.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.5820, max=0.6562, mean=-0.0000, std=0.0477
INFO:utils:model.layers.0.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.0618, max=0.0698, mean=-0.0000, std=0.0091
INFO:utils:model.layers.0.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3105, max=0.3203, mean=-0.0000, std=0.0116
INFO:utils:model.layers.0.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.6094, max=0.4023, mean=0.0000, std=0.0197
INFO:utils:model.layers.0.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3027, max=0.1934, mean=-0.0000, std=0.0175
INFO:utils:model.layers.0.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.6250, max=0.4395, mean=-0.0000, std=0.0174
INFO:utils:model.layers.0.input_layernorm.weight: shape=(2048,), dtype=float32, min=-0.6289, max=1.0781, mean=0.1443, std=0.1635
INFO:utils:model.layers.0.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=-0.2275, max=0.4160, mean=0.2093, std=0.0278
INFO:utils:model.layers.1.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3945, max=0.4141, mean=-0.0000, std=0.0285
INFO:utils:model.layers.1.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.3086, max=0.2930, mean=0.0000, std=0.0406
INFO:utils:model.layers.1.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.0737, max=0.0918, mean=0.0000, std=0.0098
INFO:utils:model.layers.1.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.5391, max=0.4629, mean=0.0000, std=0.0129
INFO:utils:model.layers.1.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.6641, max=0.8438, mean=0.0000, std=0.0205
INFO:utils:model.layers.1.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-1.1250, max=1.0234, mean=0.0000, std=0.0174
INFO:utils:model.layers.1.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.7109, max=0.6094, mean=-0.0000, std=0.0172
INFO:utils:model.layers.1.input_layernorm.weight: shape=(2048,), dtype=float32, min=-0.1631, max=0.8984, mean=0.3419, std=0.0900
INFO:utils:model.layers.1.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=-0.1895, max=0.8008, mean=0.2620, std=0.0364
INFO:utils:model.layers.2.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.5703, max=0.4863, mean=-0.0000, std=0.0280
INFO:utils:model.layers.2.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.4043, max=0.5352, mean=0.0000, std=0.0382
INFO:utils:model.layers.2.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.0850, max=0.0869, mean=0.0000, std=0.0117
INFO:utils:model.layers.2.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3418, max=0.3535, mean=-0.0000, std=0.0131
INFO:utils:model.layers.2.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3125, max=0.3398, mean=-0.0000, std=0.0211
INFO:utils:model.layers.2.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.4648, max=0.2334, mean=-0.0000, std=0.0166
INFO:utils:model.layers.2.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.5312, max=0.4219, mean=-0.0000, std=0.0168
INFO:utils:model.layers.2.input_layernorm.weight: shape=(2048,), dtype=float32, min=-0.3203, max=1.0469, mean=0.4376, std=0.0991
INFO:utils:model.layers.2.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=-0.2256, max=0.4160, mean=0.3015, std=0.0397
INFO:utils:model.layers.3.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3086, max=0.2852, mean=0.0000, std=0.0276
INFO:utils:model.layers.3.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.2812, max=0.2910, mean=0.0000, std=0.0371
INFO:utils:model.layers.3.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.2021, max=0.1157, mean=0.0000, std=0.0139
INFO:utils:model.layers.3.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.2480, max=0.2432, mean=0.0000, std=0.0149
INFO:utils:model.layers.3.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3164, max=0.3340, mean=-0.0000, std=0.0230
INFO:utils:model.layers.3.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3477, max=0.3008, mean=-0.0000, std=0.0161
INFO:utils:model.layers.3.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.4316, max=0.5078, mean=0.0000, std=0.0160
INFO:utils:model.layers.3.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1025, max=0.9844, mean=0.4067, std=0.0517
INFO:utils:model.layers.3.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=-0.2031, max=0.4297, mean=0.3156, std=0.0305
INFO:utils:model.layers.4.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3789, max=0.3477, mean=0.0000, std=0.0267
INFO:utils:model.layers.4.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.4258, max=0.3672, mean=0.0000, std=0.0366
INFO:utils:model.layers.4.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1240, max=0.1084, mean=-0.0000, std=0.0134
INFO:utils:model.layers.4.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3418, max=0.4062, mean=-0.0000, std=0.0149
INFO:utils:model.layers.4.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3477, max=0.5117, mean=-0.0001, std=0.0237
INFO:utils:model.layers.4.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.2656, max=0.2773, mean=0.0000, std=0.0160
INFO:utils:model.layers.4.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.4023, max=0.4785, mean=-0.0000, std=0.0158
INFO:utils:model.layers.4.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1187, max=0.9062, mean=0.4146, std=0.0548
INFO:utils:model.layers.4.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=-0.2480, max=0.3945, mean=0.3244, std=0.0371
INFO:utils:model.layers.5.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.8008, max=0.5352, mean=0.0000, std=0.0260
INFO:utils:model.layers.5.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-1.1328, max=1.0781, mean=-0.0000, std=0.0370
INFO:utils:model.layers.5.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1387, max=0.0918, mean=-0.0000, std=0.0112
INFO:utils:model.layers.5.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3535, max=0.4648, mean=0.0000, std=0.0135
INFO:utils:model.layers.5.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3340, max=0.3047, mean=-0.0001, std=0.0222
INFO:utils:model.layers.5.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3594, max=0.3555, mean=0.0000, std=0.0162
INFO:utils:model.layers.5.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.4766, max=0.4258, mean=0.0000, std=0.0160
INFO:utils:model.layers.5.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1182, max=1.1250, mean=0.4701, std=0.0803
INFO:utils:model.layers.5.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=-0.0000, max=0.4434, mean=0.3337, std=0.0396
INFO:utils:model.layers.6.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.2197, max=0.2129, mean=-0.0000, std=0.0246
INFO:utils:model.layers.6.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.3867, max=0.3711, mean=-0.0000, std=0.0382
INFO:utils:model.layers.6.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1270, max=0.1582, mean=-0.0000, std=0.0132
INFO:utils:model.layers.6.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.2715, max=0.2490, mean=-0.0000, std=0.0143
INFO:utils:model.layers.6.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3574, max=0.3730, mean=-0.0001, std=0.0217
INFO:utils:model.layers.6.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.4648, max=0.2578, mean=0.0000, std=0.0160
INFO:utils:model.layers.6.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.6445, max=0.5273, mean=0.0000, std=0.0157
INFO:utils:model.layers.6.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1514, max=0.9297, mean=0.4650, std=0.0623
INFO:utils:model.layers.6.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=-0.1641, max=0.4609, mean=0.3329, std=0.0377
INFO:utils:model.layers.7.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.2637, max=0.3027, mean=0.0000, std=0.0264
INFO:utils:model.layers.7.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.3340, max=0.2930, mean=-0.0000, std=0.0380
INFO:utils:model.layers.7.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.0918, max=0.0869, mean=0.0000, std=0.0137
INFO:utils:model.layers.7.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3691, max=0.2520, mean=-0.0000, std=0.0147
INFO:utils:model.layers.7.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.5938, max=0.5078, mean=-0.0001, std=0.0210
INFO:utils:model.layers.7.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.2227, max=0.2246, mean=0.0000, std=0.0165
INFO:utils:model.layers.7.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.4844, max=0.4688, mean=-0.0000, std=0.0161
INFO:utils:model.layers.7.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1387, max=1.0078, mean=0.4750, std=0.0745
INFO:utils:model.layers.7.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.1338, max=0.4570, mean=0.3340, std=0.0307
INFO:utils:model.layers.8.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.4785, max=0.4941, mean=-0.0000, std=0.0255
INFO:utils:model.layers.8.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.5117, max=0.4570, mean=0.0000, std=0.0385
INFO:utils:model.layers.8.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.0879, max=0.0796, mean=-0.0000, std=0.0129
INFO:utils:model.layers.8.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3926, max=0.3125, mean=0.0000, std=0.0146
INFO:utils:model.layers.8.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.4629, max=0.5508, mean=-0.0001, std=0.0207
INFO:utils:model.layers.8.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3105, max=0.2490, mean=-0.0000, std=0.0166
INFO:utils:model.layers.8.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.4668, max=0.5820, mean=-0.0000, std=0.0162
INFO:utils:model.layers.8.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1416, max=1.0938, mean=0.5031, std=0.0719
INFO:utils:model.layers.8.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.1182, max=0.5156, mean=0.3534, std=0.0210
INFO:utils:model.layers.9.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3672, max=0.4258, mean=0.0000, std=0.0297
INFO:utils:model.layers.9.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.3574, max=0.3926, mean=0.0000, std=0.0369
INFO:utils:model.layers.9.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1250, max=0.1001, mean=0.0000, std=0.0141
INFO:utils:model.layers.9.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.4473, max=0.5039, mean=0.0000, std=0.0157
INFO:utils:model.layers.9.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.5117, max=0.4961, mean=-0.0001, std=0.0217
INFO:utils:model.layers.9.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.2520, max=0.2471, mean=0.0000, std=0.0171
INFO:utils:model.layers.9.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.3418, max=0.5664, mean=-0.0000, std=0.0166
INFO:utils:model.layers.9.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1270, max=1.0391, mean=0.4824, std=0.0616
INFO:utils:model.layers.9.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.0903, max=0.5000, mean=0.3702, std=0.0259
INFO:utils:model.layers.10.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3184, max=0.3965, mean=-0.0001, std=0.0269
INFO:utils:model.layers.10.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.2871, max=0.2949, mean=0.0000, std=0.0348
INFO:utils:model.layers.10.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1680, max=0.1670, mean=0.0000, std=0.0125
INFO:utils:model.layers.10.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.5156, max=0.6094, mean=-0.0000, std=0.0145
INFO:utils:model.layers.10.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.5781, max=0.4492, mean=-0.0000, std=0.0216
INFO:utils:model.layers.10.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.5469, max=0.2930, mean=-0.0000, std=0.0172
INFO:utils:model.layers.10.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.3887, max=0.5312, mean=0.0000, std=0.0169
INFO:utils:model.layers.10.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1108, max=1.1172, mean=0.5507, std=0.0723
INFO:utils:model.layers.10.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.0894, max=0.5273, mean=0.4114, std=0.0397
INFO:utils:model.layers.11.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.2637, max=0.2217, mean=-0.0000, std=0.0273
INFO:utils:model.layers.11.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.4160, max=0.4375, mean=0.0001, std=0.0377
INFO:utils:model.layers.11.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1270, max=0.1318, mean=0.0000, std=0.0134
INFO:utils:model.layers.11.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.6484, max=0.5156, mean=0.0000, std=0.0151
INFO:utils:model.layers.11.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.4727, max=0.6406, mean=-0.0000, std=0.0219
INFO:utils:model.layers.11.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.3047, max=0.3203, mean=0.0000, std=0.0176
INFO:utils:model.layers.11.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.3418, max=0.3848, mean=-0.0000, std=0.0173
INFO:utils:model.layers.11.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.1055, max=0.9922, mean=0.5220, std=0.0589
INFO:utils:model.layers.11.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.1099, max=0.5312, mean=0.4462, std=0.0528
INFO:utils:model.layers.12.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3770, max=0.3457, mean=-0.0000, std=0.0273
INFO:utils:model.layers.12.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.7539, max=0.5156, mean=-0.0000, std=0.0362
INFO:utils:model.layers.12.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1309, max=0.1797, mean=-0.0000, std=0.0144
INFO:utils:model.layers.12.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.5742, max=0.6992, mean=-0.0000, std=0.0154
INFO:utils:model.layers.12.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.5156, max=0.6055, mean=-0.0000, std=0.0218
INFO:utils:model.layers.12.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.2305, max=0.2598, mean=-0.0000, std=0.0180
INFO:utils:model.layers.12.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.3711, max=0.6289, mean=0.0000, std=0.0177
INFO:utils:model.layers.12.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.0889, max=1.0000, mean=0.4996, std=0.0606
INFO:utils:model.layers.12.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.0967, max=0.5195, mean=0.4679, std=0.0584
INFO:utils:model.layers.13.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3164, max=0.2930, mean=0.0000, std=0.0265
INFO:utils:model.layers.13.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.3945, max=0.5352, mean=0.0000, std=0.0334
INFO:utils:model.layers.13.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.1729, max=0.1699, mean=0.0000, std=0.0171
INFO:utils:model.layers.13.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.4805, max=0.3848, mean=-0.0000, std=0.0172
INFO:utils:model.layers.13.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.6211, max=0.6680, mean=-0.0000, std=0.0213
INFO:utils:model.layers.13.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.4473, max=0.3535, mean=0.0000, std=0.0185
INFO:utils:model.layers.13.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.4043, max=0.4590, mean=-0.0000, std=0.0181
INFO:utils:model.layers.13.input_layernorm.weight: shape=(2048,), dtype=float32, min=-0.0913, max=1.1875, mean=0.5346, std=0.0850
INFO:utils:model.layers.13.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.1084, max=0.5312, mean=0.4751, std=0.0530
INFO:utils:model.layers.14.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.4609, max=0.4258, mean=0.0000, std=0.0268
INFO:utils:model.layers.14.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.5820, max=0.5234, mean=0.0001, std=0.0305
INFO:utils:model.layers.14.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.3027, max=0.3027, mean=0.0000, std=0.0227
INFO:utils:model.layers.14.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3594, max=0.3652, mean=0.0000, std=0.0201
INFO:utils:model.layers.14.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.8008, max=0.7266, mean=-0.0001, std=0.0221
INFO:utils:model.layers.14.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.4941, max=0.4102, mean=-0.0000, std=0.0185
INFO:utils:model.layers.14.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.3828, max=0.4531, mean=0.0000, std=0.0178
INFO:utils:model.layers.14.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.0781, max=1.2500, mean=0.4703, std=0.1198
INFO:utils:model.layers.14.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.0850, max=0.6328, mean=0.5047, std=0.0584
INFO:utils:model.layers.15.self_attn.q_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.3223, max=0.3340, mean=0.0000, std=0.0249
INFO:utils:model.layers.15.self_attn.k_proj.weight: shape=(512, 2048), dtype=float32, min=-0.4902, max=0.4961, mean=0.0001, std=0.0302
INFO:utils:model.layers.15.self_attn.v_proj.weight: shape=(512, 2048), dtype=float32, min=-0.3008, max=0.2559, mean=-0.0000, std=0.0217
INFO:utils:model.layers.15.self_attn.o_proj.weight: shape=(2048, 2048), dtype=float32, min=-0.6953, max=0.7344, mean=0.0000, std=0.0197
INFO:utils:model.layers.15.mlp.gate_proj.weight: shape=(8192, 2048), dtype=float32, min=-0.9961, max=0.8555, mean=-0.0000, std=0.0233
INFO:utils:model.layers.15.mlp.up_proj.weight: shape=(8192, 2048), dtype=float32, min=-1.2344, max=1.0234, mean=-0.0000, std=0.0202
INFO:utils:model.layers.15.mlp.down_proj.weight: shape=(2048, 8192), dtype=float32, min=-0.8477, max=0.8672, mean=-0.0000, std=0.0178
INFO:utils:model.layers.15.input_layernorm.weight: shape=(2048,), dtype=float32, min=0.0835, max=1.1641, mean=0.5031, std=0.1247
INFO:utils:model.layers.15.post_attention_layernorm.weight: shape=(2048,), dtype=float32, min=0.0576, max=0.6445, mean=0.5089, std=0.0638
INFO:utils:model.norm.weight: shape=(2048,), dtype=float32, min=0.0306, max=2.9062, mean=2.3526, std=0.2831
INFO:utils:model.embed_tokens.weight (converted): dtype=float32, min=-0.3027, max=0.3457, mean=-0.0001, std=0.0219
INFO:utils:model.layers.0.self_attn.q_proj.weight (converted): dtype=float32, min=-0.7070, max=0.5977, mean=0.0000, std=0.0369
INFO:utils:model.layers.0.self_attn.k_proj.weight (converted): dtype=float32, min=-0.5820, max=0.6562, mean=-0.0000, std=0.0477
INFO:utils:model.layers.0.self_attn.v_proj.weight (converted): dtype=float32, min=-0.0618, max=0.0698, mean=-0.0000, std=0.0091
INFO:utils:model.layers.0.self_attn.o_proj.weight (converted): dtype=float32, min=-0.3105, max=0.3203, mean=-0.0000, std=0.0116
INFO:utils:model.layers.0.mlp.gate_proj.weight (converted): dtype=float32, min=-0.6094, max=0.4023, mean=0.0000, std=0.0197
INFO:utils:model.layers.0.mlp.up_proj.weight (converted): dtype=float32, min=-0.3027, max=0.1934, mean=-0.0000, std=0.0175
INFO:utils:model.layers.0.mlp.down_proj.weight (converted): dtype=float32, min=-0.6250, max=0.4395, mean=-0.0000, std=0.0174
INFO:utils:model.layers.0.input_layernorm.weight (converted): dtype=float32, min=-0.6289, max=1.0781, mean=0.1443, std=0.1635
INFO:utils:model.layers.0.post_attention_layernorm.weight (converted): dtype=float32, min=-0.2275, max=0.4160, mean=0.2093, std=0.0278
INFO:utils:model.layers.1.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3945, max=0.4141, mean=-0.0000, std=0.0285
INFO:utils:model.layers.1.self_attn.k_proj.weight (converted): dtype=float32, min=-0.3086, max=0.2930, mean=0.0000, std=0.0406
INFO:utils:model.layers.1.self_attn.v_proj.weight (converted): dtype=float32, min=-0.0737, max=0.0918, mean=0.0000, std=0.0098
INFO:utils:model.layers.1.self_attn.o_proj.weight (converted): dtype=float32, min=-0.5391, max=0.4629, mean=0.0000, std=0.0129
INFO:utils:model.layers.1.mlp.gate_proj.weight (converted): dtype=float32, min=-0.6641, max=0.8438, mean=0.0000, std=0.0205
INFO:utils:model.layers.1.mlp.up_proj.weight (converted): dtype=float32, min=-1.1250, max=1.0234, mean=0.0000, std=0.0174
INFO:utils:model.layers.1.mlp.down_proj.weight (converted): dtype=float32, min=-0.7109, max=0.6094, mean=-0.0000, std=0.0172
INFO:utils:model.layers.1.input_layernorm.weight (converted): dtype=float32, min=-0.1631, max=0.8984, mean=0.3419, std=0.0900
INFO:utils:model.layers.1.post_attention_layernorm.weight (converted): dtype=float32, min=-0.1895, max=0.8008, mean=0.2620, std=0.0364
INFO:utils:model.layers.2.self_attn.q_proj.weight (converted): dtype=float32, min=-0.5703, max=0.4863, mean=-0.0000, std=0.0280
INFO:utils:model.layers.2.self_attn.k_proj.weight (converted): dtype=float32, min=-0.4043, max=0.5352, mean=0.0000, std=0.0382
INFO:utils:model.layers.2.self_attn.v_proj.weight (converted): dtype=float32, min=-0.0850, max=0.0869, mean=0.0000, std=0.0117
INFO:utils:model.layers.2.self_attn.o_proj.weight (converted): dtype=float32, min=-0.3418, max=0.3535, mean=-0.0000, std=0.0131
INFO:utils:model.layers.2.mlp.gate_proj.weight (converted): dtype=float32, min=-0.3125, max=0.3398, mean=-0.0000, std=0.0211
INFO:utils:model.layers.2.mlp.up_proj.weight (converted): dtype=float32, min=-0.4648, max=0.2334, mean=-0.0000, std=0.0166
INFO:utils:model.layers.2.mlp.down_proj.weight (converted): dtype=float32, min=-0.5312, max=0.4219, mean=-0.0000, std=0.0168
INFO:utils:model.layers.2.input_layernorm.weight (converted): dtype=float32, min=-0.3203, max=1.0469, mean=0.4376, std=0.0991
INFO:utils:model.layers.2.post_attention_layernorm.weight (converted): dtype=float32, min=-0.2256, max=0.4160, mean=0.3015, std=0.0397
INFO:utils:model.layers.3.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3086, max=0.2852, mean=0.0000, std=0.0276
INFO:utils:model.layers.3.self_attn.k_proj.weight (converted): dtype=float32, min=-0.2812, max=0.2910, mean=0.0000, std=0.0371
INFO:utils:model.layers.3.self_attn.v_proj.weight (converted): dtype=float32, min=-0.2021, max=0.1157, mean=0.0000, std=0.0139
INFO:utils:model.layers.3.self_attn.o_proj.weight (converted): dtype=float32, min=-0.2480, max=0.2432, mean=0.0000, std=0.0149
INFO:utils:model.layers.3.mlp.gate_proj.weight (converted): dtype=float32, min=-0.3164, max=0.3340, mean=-0.0000, std=0.0230
INFO:utils:model.layers.3.mlp.up_proj.weight (converted): dtype=float32, min=-0.3477, max=0.3008, mean=-0.0000, std=0.0161
INFO:utils:model.layers.3.mlp.down_proj.weight (converted): dtype=float32, min=-0.4316, max=0.5078, mean=0.0000, std=0.0160
INFO:utils:model.layers.3.input_layernorm.weight (converted): dtype=float32, min=0.1025, max=0.9844, mean=0.4067, std=0.0517
INFO:utils:model.layers.3.post_attention_layernorm.weight (converted): dtype=float32, min=-0.2031, max=0.4297, mean=0.3156, std=0.0305
INFO:utils:model.layers.4.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3789, max=0.3477, mean=0.0000, std=0.0267
INFO:utils:model.layers.4.self_attn.k_proj.weight (converted): dtype=float32, min=-0.4258, max=0.3672, mean=0.0000, std=0.0366
INFO:utils:model.layers.4.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1240, max=0.1084, mean=-0.0000, std=0.0134
INFO:utils:model.layers.4.self_attn.o_proj.weight (converted): dtype=float32, min=-0.3418, max=0.4062, mean=-0.0000, std=0.0149
INFO:utils:model.layers.4.mlp.gate_proj.weight (converted): dtype=float32, min=-0.3477, max=0.5117, mean=-0.0001, std=0.0237
INFO:utils:model.layers.4.mlp.up_proj.weight (converted): dtype=float32, min=-0.2656, max=0.2773, mean=0.0000, std=0.0160
INFO:utils:model.layers.4.mlp.down_proj.weight (converted): dtype=float32, min=-0.4023, max=0.4785, mean=-0.0000, std=0.0158
INFO:utils:model.layers.4.input_layernorm.weight (converted): dtype=float32, min=0.1187, max=0.9062, mean=0.4146, std=0.0548
INFO:utils:model.layers.4.post_attention_layernorm.weight (converted): dtype=float32, min=-0.2480, max=0.3945, mean=0.3244, std=0.0371
INFO:utils:model.layers.5.self_attn.q_proj.weight (converted): dtype=float32, min=-0.8008, max=0.5352, mean=0.0000, std=0.0260
INFO:utils:model.layers.5.self_attn.k_proj.weight (converted): dtype=float32, min=-1.1328, max=1.0781, mean=-0.0000, std=0.0370
INFO:utils:model.layers.5.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1387, max=0.0918, mean=-0.0000, std=0.0112
INFO:utils:model.layers.5.self_attn.o_proj.weight (converted): dtype=float32, min=-0.3535, max=0.4648, mean=0.0000, std=0.0135
INFO:utils:model.layers.5.mlp.gate_proj.weight (converted): dtype=float32, min=-0.3340, max=0.3047, mean=-0.0001, std=0.0222
INFO:utils:model.layers.5.mlp.up_proj.weight (converted): dtype=float32, min=-0.3594, max=0.3555, mean=0.0000, std=0.0162
INFO:utils:model.layers.5.mlp.down_proj.weight (converted): dtype=float32, min=-0.4766, max=0.4258, mean=0.0000, std=0.0160
INFO:utils:model.layers.5.input_layernorm.weight (converted): dtype=float32, min=0.1182, max=1.1250, mean=0.4701, std=0.0803
INFO:utils:model.layers.5.post_attention_layernorm.weight (converted): dtype=float32, min=-0.0000, max=0.4434, mean=0.3337, std=0.0396
INFO:utils:model.layers.6.self_attn.q_proj.weight (converted): dtype=float32, min=-0.2197, max=0.2129, mean=-0.0000, std=0.0246
INFO:utils:model.layers.6.self_attn.k_proj.weight (converted): dtype=float32, min=-0.3867, max=0.3711, mean=-0.0000, std=0.0382
INFO:utils:model.layers.6.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1270, max=0.1582, mean=-0.0000, std=0.0132
INFO:utils:model.layers.6.self_attn.o_proj.weight (converted): dtype=float32, min=-0.2715, max=0.2490, mean=-0.0000, std=0.0143
INFO:utils:model.layers.6.mlp.gate_proj.weight (converted): dtype=float32, min=-0.3574, max=0.3730, mean=-0.0001, std=0.0217
INFO:utils:model.layers.6.mlp.up_proj.weight (converted): dtype=float32, min=-0.4648, max=0.2578, mean=0.0000, std=0.0160
INFO:utils:model.layers.6.mlp.down_proj.weight (converted): dtype=float32, min=-0.6445, max=0.5273, mean=0.0000, std=0.0157
INFO:utils:model.layers.6.input_layernorm.weight (converted): dtype=float32, min=0.1514, max=0.9297, mean=0.4650, std=0.0623
INFO:utils:model.layers.6.post_attention_layernorm.weight (converted): dtype=float32, min=-0.1641, max=0.4609, mean=0.3329, std=0.0377
INFO:utils:model.layers.7.self_attn.q_proj.weight (converted): dtype=float32, min=-0.2637, max=0.3027, mean=0.0000, std=0.0264
INFO:utils:model.layers.7.self_attn.k_proj.weight (converted): dtype=float32, min=-0.3340, max=0.2930, mean=-0.0000, std=0.0380
INFO:utils:model.layers.7.self_attn.v_proj.weight (converted): dtype=float32, min=-0.0918, max=0.0869, mean=0.0000, std=0.0137
INFO:utils:model.layers.7.self_attn.o_proj.weight (converted): dtype=float32, min=-0.3691, max=0.2520, mean=-0.0000, std=0.0147
INFO:utils:model.layers.7.mlp.gate_proj.weight (converted): dtype=float32, min=-0.5938, max=0.5078, mean=-0.0001, std=0.0210
INFO:utils:model.layers.7.mlp.up_proj.weight (converted): dtype=float32, min=-0.2227, max=0.2246, mean=0.0000, std=0.0165
INFO:utils:model.layers.7.mlp.down_proj.weight (converted): dtype=float32, min=-0.4844, max=0.4688, mean=-0.0000, std=0.0161
INFO:utils:model.layers.7.input_layernorm.weight (converted): dtype=float32, min=0.1387, max=1.0078, mean=0.4750, std=0.0745
INFO:utils:model.layers.7.post_attention_layernorm.weight (converted): dtype=float32, min=0.1338, max=0.4570, mean=0.3340, std=0.0307
INFO:utils:model.layers.8.self_attn.q_proj.weight (converted): dtype=float32, min=-0.4785, max=0.4941, mean=-0.0000, std=0.0255
INFO:utils:model.layers.8.self_attn.k_proj.weight (converted): dtype=float32, min=-0.5117, max=0.4570, mean=0.0000, std=0.0385
INFO:utils:model.layers.8.self_attn.v_proj.weight (converted): dtype=float32, min=-0.0879, max=0.0796, mean=-0.0000, std=0.0129
INFO:utils:model.layers.8.self_attn.o_proj.weight (converted): dtype=float32, min=-0.3926, max=0.3125, mean=0.0000, std=0.0146
INFO:utils:model.layers.8.mlp.gate_proj.weight (converted): dtype=float32, min=-0.4629, max=0.5508, mean=-0.0001, std=0.0207
INFO:utils:model.layers.8.mlp.up_proj.weight (converted): dtype=float32, min=-0.3105, max=0.2490, mean=-0.0000, std=0.0166
INFO:utils:model.layers.8.mlp.down_proj.weight (converted): dtype=float32, min=-0.4668, max=0.5820, mean=-0.0000, std=0.0162
INFO:utils:model.layers.8.input_layernorm.weight (converted): dtype=float32, min=0.1416, max=1.0938, mean=0.5031, std=0.0719
INFO:utils:model.layers.8.post_attention_layernorm.weight (converted): dtype=float32, min=0.1182, max=0.5156, mean=0.3534, std=0.0210
INFO:utils:model.layers.9.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3672, max=0.4258, mean=0.0000, std=0.0297
INFO:utils:model.layers.9.self_attn.k_proj.weight (converted): dtype=float32, min=-0.3574, max=0.3926, mean=0.0000, std=0.0369
INFO:utils:model.layers.9.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1250, max=0.1001, mean=0.0000, std=0.0141
INFO:utils:model.layers.9.self_attn.o_proj.weight (converted): dtype=float32, min=-0.4473, max=0.5039, mean=0.0000, std=0.0157
INFO:utils:model.layers.9.mlp.gate_proj.weight (converted): dtype=float32, min=-0.5117, max=0.4961, mean=-0.0001, std=0.0217
INFO:utils:model.layers.9.mlp.up_proj.weight (converted): dtype=float32, min=-0.2520, max=0.2471, mean=0.0000, std=0.0171
INFO:utils:model.layers.9.mlp.down_proj.weight (converted): dtype=float32, min=-0.3418, max=0.5664, mean=-0.0000, std=0.0166
INFO:utils:model.layers.9.input_layernorm.weight (converted): dtype=float32, min=0.1270, max=1.0391, mean=0.4824, std=0.0616
INFO:utils:model.layers.9.post_attention_layernorm.weight (converted): dtype=float32, min=0.0903, max=0.5000, mean=0.3702, std=0.0259
INFO:utils:model.layers.10.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3184, max=0.3965, mean=-0.0001, std=0.0269
INFO:utils:model.layers.10.self_attn.k_proj.weight (converted): dtype=float32, min=-0.2871, max=0.2949, mean=0.0000, std=0.0348
INFO:utils:model.layers.10.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1680, max=0.1670, mean=0.0000, std=0.0125
INFO:utils:model.layers.10.self_attn.o_proj.weight (converted): dtype=float32, min=-0.5156, max=0.6094, mean=-0.0000, std=0.0145
INFO:utils:model.layers.10.mlp.gate_proj.weight (converted): dtype=float32, min=-0.5781, max=0.4492, mean=-0.0000, std=0.0216
INFO:utils:model.layers.10.mlp.up_proj.weight (converted): dtype=float32, min=-0.5469, max=0.2930, mean=-0.0000, std=0.0172
INFO:utils:model.layers.10.mlp.down_proj.weight (converted): dtype=float32, min=-0.3887, max=0.5312, mean=0.0000, std=0.0169
INFO:utils:model.layers.10.input_layernorm.weight (converted): dtype=float32, min=0.1108, max=1.1172, mean=0.5507, std=0.0723
INFO:utils:model.layers.10.post_attention_layernorm.weight (converted): dtype=float32, min=0.0894, max=0.5273, mean=0.4114, std=0.0397
INFO:utils:model.layers.11.self_attn.q_proj.weight (converted): dtype=float32, min=-0.2637, max=0.2217, mean=-0.0000, std=0.0273
INFO:utils:model.layers.11.self_attn.k_proj.weight (converted): dtype=float32, min=-0.4160, max=0.4375, mean=0.0001, std=0.0377
INFO:utils:model.layers.11.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1270, max=0.1318, mean=0.0000, std=0.0134
INFO:utils:model.layers.11.self_attn.o_proj.weight (converted): dtype=float32, min=-0.6484, max=0.5156, mean=0.0000, std=0.0151
INFO:utils:model.layers.11.mlp.gate_proj.weight (converted): dtype=float32, min=-0.4727, max=0.6406, mean=-0.0000, std=0.0219
INFO:utils:model.layers.11.mlp.up_proj.weight (converted): dtype=float32, min=-0.3047, max=0.3203, mean=0.0000, std=0.0176
INFO:utils:model.layers.11.mlp.down_proj.weight (converted): dtype=float32, min=-0.3418, max=0.3848, mean=-0.0000, std=0.0173
INFO:utils:model.layers.11.input_layernorm.weight (converted): dtype=float32, min=0.1055, max=0.9922, mean=0.5220, std=0.0589
INFO:utils:model.layers.11.post_attention_layernorm.weight (converted): dtype=float32, min=0.1099, max=0.5312, mean=0.4462, std=0.0528
INFO:utils:model.layers.12.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3770, max=0.3457, mean=-0.0000, std=0.0273
INFO:utils:model.layers.12.self_attn.k_proj.weight (converted): dtype=float32, min=-0.7539, max=0.5156, mean=-0.0000, std=0.0362
INFO:utils:model.layers.12.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1309, max=0.1797, mean=-0.0000, std=0.0144
INFO:utils:model.layers.12.self_attn.o_proj.weight (converted): dtype=float32, min=-0.5742, max=0.6992, mean=-0.0000, std=0.0154
INFO:utils:model.layers.12.mlp.gate_proj.weight (converted): dtype=float32, min=-0.5156, max=0.6055, mean=-0.0000, std=0.0218
INFO:utils:model.layers.12.mlp.up_proj.weight (converted): dtype=float32, min=-0.2305, max=0.2598, mean=-0.0000, std=0.0180
INFO:utils:model.layers.12.mlp.down_proj.weight (converted): dtype=float32, min=-0.3711, max=0.6289, mean=0.0000, std=0.0177
INFO:utils:model.layers.12.input_layernorm.weight (converted): dtype=float32, min=0.0889, max=1.0000, mean=0.4996, std=0.0606
INFO:utils:model.layers.12.post_attention_layernorm.weight (converted): dtype=float32, min=0.0967, max=0.5195, mean=0.4679, std=0.0584
INFO:utils:model.layers.13.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3164, max=0.2930, mean=0.0000, std=0.0265
INFO:utils:model.layers.13.self_attn.k_proj.weight (converted): dtype=float32, min=-0.3945, max=0.5352, mean=0.0000, std=0.0334
INFO:utils:model.layers.13.self_attn.v_proj.weight (converted): dtype=float32, min=-0.1729, max=0.1699, mean=0.0000, std=0.0171
INFO:utils:model.layers.13.self_attn.o_proj.weight (converted): dtype=float32, min=-0.4805, max=0.3848, mean=-0.0000, std=0.0172
INFO:utils:model.layers.13.mlp.gate_proj.weight (converted): dtype=float32, min=-0.6211, max=0.6680, mean=-0.0000, std=0.0213
INFO:utils:model.layers.13.mlp.up_proj.weight (converted): dtype=float32, min=-0.4473, max=0.3535, mean=0.0000, std=0.0185
INFO:utils:model.layers.13.mlp.down_proj.weight (converted): dtype=float32, min=-0.4043, max=0.4590, mean=-0.0000, std=0.0181
INFO:utils:model.layers.13.input_layernorm.weight (converted): dtype=float32, min=-0.0913, max=1.1875, mean=0.5346, std=0.0850
INFO:utils:model.layers.13.post_attention_layernorm.weight (converted): dtype=float32, min=0.1084, max=0.5312, mean=0.4751, std=0.0530
INFO:utils:model.layers.14.self_attn.q_proj.weight (converted): dtype=float32, min=-0.4609, max=0.4258, mean=0.0000, std=0.0268
INFO:utils:model.layers.14.self_attn.k_proj.weight (converted): dtype=float32, min=-0.5820, max=0.5234, mean=0.0001, std=0.0305
INFO:utils:model.layers.14.self_attn.v_proj.weight (converted): dtype=float32, min=-0.3027, max=0.3027, mean=0.0000, std=0.0227
INFO:utils:model.layers.14.self_attn.o_proj.weight (converted): dtype=float32, min=-0.3594, max=0.3652, mean=0.0000, std=0.0201
INFO:utils:model.layers.14.mlp.gate_proj.weight (converted): dtype=float32, min=-0.8008, max=0.7266, mean=-0.0001, std=0.0221
INFO:utils:model.layers.14.mlp.up_proj.weight (converted): dtype=float32, min=-0.4941, max=0.4102, mean=-0.0000, std=0.0185
INFO:utils:model.layers.14.mlp.down_proj.weight (converted): dtype=float32, min=-0.3828, max=0.4531, mean=0.0000, std=0.0178
INFO:utils:model.layers.14.input_layernorm.weight (converted): dtype=float32, min=0.0781, max=1.2500, mean=0.4703, std=0.1198
INFO:utils:model.layers.14.post_attention_layernorm.weight (converted): dtype=float32, min=0.0850, max=0.6328, mean=0.5047, std=0.0584
INFO:utils:model.layers.15.self_attn.q_proj.weight (converted): dtype=float32, min=-0.3223, max=0.3340, mean=0.0000, std=0.0249
INFO:utils:model.layers.15.self_attn.k_proj.weight (converted): dtype=float32, min=-0.4902, max=0.4961, mean=0.0001, std=0.0302
INFO:utils:model.layers.15.self_attn.v_proj.weight (converted): dtype=float32, min=-0.3008, max=0.2559, mean=-0.0000, std=0.0217
INFO:utils:model.layers.15.self_attn.o_proj.weight (converted): dtype=float32, min=-0.6953, max=0.7344, mean=0.0000, std=0.0197
INFO:utils:model.layers.15.mlp.gate_proj.weight (converted): dtype=float32, min=-0.9961, max=0.8555, mean=-0.0000, std=0.0233
INFO:utils:model.layers.15.mlp.up_proj.weight (converted): dtype=float32, min=-1.2344, max=1.0234, mean=-0.0000, std=0.0202
INFO:utils:model.layers.15.mlp.down_proj.weight (converted): dtype=float32, min=-0.8477, max=0.8672, mean=-0.0000, std=0.0178
INFO:utils:model.layers.15.input_layernorm.weight (converted): dtype=float32, min=0.0835, max=1.1641, mean=0.5031, std=0.1247
INFO:utils:model.layers.15.post_attention_layernorm.weight (converted): dtype=float32, min=0.0576, max=0.6445, mean=0.5089, std=0.0638
INFO:utils:model.norm.weight (converted): dtype=float32, min=0.0306, max=2.9062, mean=2.3526, std=0.2831

Once upon a time, there was a man who was a man who was a man who was a man who was a man who was a man who was a man was a time time time time time time time time time time time time, and a time and a, and, who, who, who, who, who, who, a who, a, a a who a who a a a a a a a a a a a a a a a a a a a a a a who a who a who was who was who on on on on on on on on on on on on on on on on on on on on on on on on on on on on who, who was who who, who, who was a was a was was a

Token count: 155, elapsed: 12.27s, 13 tokens/s