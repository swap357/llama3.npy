(llama3.npy) (base) swap357@Mac llama3.npy % python generate_hf.py "Once upon a time"                                          
2025-03-31 23:56:21,640 - INFO - Loading tokenizer from /Users/swap357/Documents/dev/llama3.npy/llama-3.2-1B
2025-03-31 23:56:21,909 - INFO - Tokenizer initialized with vocab size: 128000
2025-03-31 23:56:21,909 - INFO - BOS ID: 128000, EOS ID: 128001
2025-03-31 23:56:21,909 - INFO - Loading model from /Users/swap357/Documents/dev/llama3.npy/llama-3.2-1B
HF Model Class: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
HF Model Config RMS Norm Epsilon: 1e-05
2025-03-31 23:56:26,769 - INFO - ==== HuggingFace Model Weight Statistics ====
2025-03-31 23:56:27,320 - INFO - model.embed_tokens.weight: shape=(128256, 2048), dtype=torch.float32, min=-0.3027, max=0.3457, mean=-0.0001, std=0.0219
2025-03-31 23:56:27,339 - INFO - model.layers.0.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.7070, max=0.5977, mean=0.0000, std=0.0369
2025-03-31 23:56:27,340 - INFO - model.layers.0.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.5820, max=0.6562, mean=-0.0000, std=0.0477
2025-03-31 23:56:27,342 - INFO - model.layers.0.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.0618, max=0.0698, mean=-0.0000, std=0.0091
2025-03-31 23:56:27,348 - INFO - model.layers.0.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3105, max=0.3203, mean=-0.0000, std=0.0116
2025-03-31 23:56:27,383 - INFO - model.layers.0.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.6094, max=0.4023, mean=0.0000, std=0.0197
2025-03-31 23:56:27,411 - INFO - model.layers.0.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3027, max=0.1934, mean=-0.0000, std=0.0175
2025-03-31 23:56:27,433 - INFO - model.layers.0.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.6250, max=0.4395, mean=-0.0000, std=0.0174
2025-03-31 23:56:27,435 - INFO - model.layers.0.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.6289, max=1.0781, mean=0.1443, std=0.1635
2025-03-31 23:56:27,436 - INFO - model.layers.0.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.2275, max=0.4160, mean=0.2093, std=0.0278
2025-03-31 23:56:27,442 - INFO - model.layers.1.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3945, max=0.4141, mean=-0.0000, std=0.0285
2025-03-31 23:56:27,445 - INFO - model.layers.1.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.3086, max=0.2930, mean=0.0000, std=0.0406
2025-03-31 23:56:27,447 - INFO - model.layers.1.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.0737, max=0.0918, mean=0.0000, std=0.0098
2025-03-31 23:56:27,452 - INFO - model.layers.1.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.5391, max=0.4629, mean=0.0000, std=0.0129
2025-03-31 23:56:27,475 - INFO - model.layers.1.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.6641, max=0.8438, mean=0.0000, std=0.0205
2025-03-31 23:56:27,503 - INFO - model.layers.1.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-1.1250, max=1.0234, mean=0.0000, std=0.0174
2025-03-31 23:56:27,525 - INFO - model.layers.1.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.7109, max=0.6094, mean=-0.0000, std=0.0172
2025-03-31 23:56:27,527 - INFO - model.layers.1.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.1631, max=0.8984, mean=0.3419, std=0.0900
2025-03-31 23:56:27,528 - INFO - model.layers.1.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.1895, max=0.8008, mean=0.2620, std=0.0364
2025-03-31 23:56:27,534 - INFO - model.layers.2.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.5703, max=0.4863, mean=-0.0000, std=0.0280
2025-03-31 23:56:27,537 - INFO - model.layers.2.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.4043, max=0.5352, mean=0.0000, std=0.0382
2025-03-31 23:56:27,539 - INFO - model.layers.2.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.0850, max=0.0869, mean=0.0000, std=0.0117
2025-03-31 23:56:27,544 - INFO - model.layers.2.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3418, max=0.3535, mean=-0.0000, std=0.0131
2025-03-31 23:56:27,567 - INFO - model.layers.2.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3125, max=0.3398, mean=-0.0000, std=0.0211
2025-03-31 23:56:27,589 - INFO - model.layers.2.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.4648, max=0.2334, mean=-0.0000, std=0.0166
2025-03-31 23:56:27,612 - INFO - model.layers.2.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.5312, max=0.4219, mean=-0.0000, std=0.0168
2025-03-31 23:56:27,614 - INFO - model.layers.2.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.3203, max=1.0469, mean=0.4376, std=0.0991
2025-03-31 23:56:27,614 - INFO - model.layers.2.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.2256, max=0.4160, mean=0.3015, std=0.0397
2025-03-31 23:56:27,620 - INFO - model.layers.3.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3086, max=0.2852, mean=0.0000, std=0.0276
2025-03-31 23:56:27,622 - INFO - model.layers.3.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.2812, max=0.2910, mean=0.0000, std=0.0371
2025-03-31 23:56:27,624 - INFO - model.layers.3.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.2021, max=0.1157, mean=0.0000, std=0.0139
2025-03-31 23:56:27,630 - INFO - model.layers.3.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.2480, max=0.2432, mean=0.0000, std=0.0149
2025-03-31 23:56:27,652 - INFO - model.layers.3.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3164, max=0.3340, mean=-0.0000, std=0.0230
2025-03-31 23:56:27,674 - INFO - model.layers.3.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3477, max=0.3008, mean=-0.0000, std=0.0161
2025-03-31 23:56:27,696 - INFO - model.layers.3.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.4316, max=0.5078, mean=0.0000, std=0.0160
2025-03-31 23:56:27,698 - INFO - model.layers.3.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1025, max=0.9844, mean=0.4067, std=0.0517
2025-03-31 23:56:27,698 - INFO - model.layers.3.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.2031, max=0.4297, mean=0.3156, std=0.0305
2025-03-31 23:56:27,704 - INFO - model.layers.4.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3789, max=0.3477, mean=0.0000, std=0.0267
2025-03-31 23:56:27,706 - INFO - model.layers.4.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.4258, max=0.3672, mean=0.0000, std=0.0366
2025-03-31 23:56:27,708 - INFO - model.layers.4.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1240, max=0.1084, mean=-0.0000, std=0.0134
2025-03-31 23:56:27,714 - INFO - model.layers.4.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3418, max=0.4062, mean=-0.0000, std=0.0149
2025-03-31 23:56:27,736 - INFO - model.layers.4.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3477, max=0.5117, mean=-0.0001, std=0.0237
2025-03-31 23:56:27,758 - INFO - model.layers.4.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.2656, max=0.2773, mean=0.0000, std=0.0160
2025-03-31 23:56:27,780 - INFO - model.layers.4.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.4023, max=0.4785, mean=-0.0000, std=0.0158
2025-03-31 23:56:27,782 - INFO - model.layers.4.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1187, max=0.9062, mean=0.4146, std=0.0548
2025-03-31 23:56:27,782 - INFO - model.layers.4.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.2480, max=0.3945, mean=0.3244, std=0.0371
2025-03-31 23:56:27,787 - INFO - model.layers.5.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.8008, max=0.5352, mean=0.0000, std=0.0260
2025-03-31 23:56:27,790 - INFO - model.layers.5.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-1.1328, max=1.0781, mean=-0.0000, std=0.0370
2025-03-31 23:56:27,792 - INFO - model.layers.5.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1387, max=0.0918, mean=-0.0000, std=0.0112
2025-03-31 23:56:27,798 - INFO - model.layers.5.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3535, max=0.4648, mean=0.0000, std=0.0135
2025-03-31 23:56:27,820 - INFO - model.layers.5.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3340, max=0.3047, mean=-0.0001, std=0.0222
2025-03-31 23:56:27,841 - INFO - model.layers.5.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3594, max=0.3555, mean=0.0000, std=0.0162
2025-03-31 23:56:27,863 - INFO - model.layers.5.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.4766, max=0.4258, mean=0.0000, std=0.0160
2025-03-31 23:56:27,865 - INFO - model.layers.5.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1182, max=1.1250, mean=0.4701, std=0.0803
2025-03-31 23:56:27,865 - INFO - model.layers.5.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.0000, max=0.4434, mean=0.3337, std=0.0396
2025-03-31 23:56:27,871 - INFO - model.layers.6.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.2197, max=0.2129, mean=-0.0000, std=0.0246
2025-03-31 23:56:27,874 - INFO - model.layers.6.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.3867, max=0.3711, mean=-0.0000, std=0.0382
2025-03-31 23:56:27,875 - INFO - model.layers.6.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1270, max=0.1582, mean=-0.0000, std=0.0132
2025-03-31 23:56:27,882 - INFO - model.layers.6.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.2715, max=0.2490, mean=-0.0000, std=0.0143
2025-03-31 23:56:27,904 - INFO - model.layers.6.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3574, max=0.3730, mean=-0.0001, std=0.0217
2025-03-31 23:56:27,926 - INFO - model.layers.6.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.4648, max=0.2578, mean=0.0000, std=0.0160
2025-03-31 23:56:27,948 - INFO - model.layers.6.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.6445, max=0.5273, mean=0.0000, std=0.0157
2025-03-31 23:56:27,950 - INFO - model.layers.6.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1514, max=0.9297, mean=0.4650, std=0.0623
2025-03-31 23:56:27,950 - INFO - model.layers.6.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.1641, max=0.4609, mean=0.3329, std=0.0377
2025-03-31 23:56:27,956 - INFO - model.layers.7.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.2637, max=0.3027, mean=0.0000, std=0.0264
2025-03-31 23:56:27,958 - INFO - model.layers.7.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.3340, max=0.2930, mean=-0.0000, std=0.0380
2025-03-31 23:56:27,960 - INFO - model.layers.7.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.0918, max=0.0869, mean=0.0000, std=0.0137
2025-03-31 23:56:27,967 - INFO - model.layers.7.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3691, max=0.2520, mean=-0.0000, std=0.0147
2025-03-31 23:56:27,991 - INFO - model.layers.7.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.5938, max=0.5078, mean=-0.0001, std=0.0210
2025-03-31 23:56:28,014 - INFO - model.layers.7.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.2227, max=0.2246, mean=0.0000, std=0.0165
2025-03-31 23:56:28,036 - INFO - model.layers.7.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.4844, max=0.4688, mean=-0.0000, std=0.0161
2025-03-31 23:56:28,038 - INFO - model.layers.7.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1387, max=1.0078, mean=0.4750, std=0.0745
2025-03-31 23:56:28,039 - INFO - model.layers.7.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1338, max=0.4570, mean=0.3340, std=0.0307
2025-03-31 23:56:28,044 - INFO - model.layers.8.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.4785, max=0.4941, mean=-0.0000, std=0.0255
2025-03-31 23:56:28,047 - INFO - model.layers.8.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.5117, max=0.4570, mean=0.0000, std=0.0385
2025-03-31 23:56:28,049 - INFO - model.layers.8.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.0879, max=0.0796, mean=-0.0000, std=0.0129
2025-03-31 23:56:28,055 - INFO - model.layers.8.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3926, max=0.3125, mean=0.0000, std=0.0146
2025-03-31 23:56:28,077 - INFO - model.layers.8.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.4629, max=0.5508, mean=-0.0001, std=0.0207
2025-03-31 23:56:28,100 - INFO - model.layers.8.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3105, max=0.2490, mean=-0.0000, std=0.0166
2025-03-31 23:56:28,123 - INFO - model.layers.8.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.4668, max=0.5820, mean=-0.0000, std=0.0162
2025-03-31 23:56:28,124 - INFO - model.layers.8.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1416, max=1.0938, mean=0.5031, std=0.0719
2025-03-31 23:56:28,125 - INFO - model.layers.8.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1182, max=0.5156, mean=0.3534, std=0.0210
2025-03-31 23:56:28,131 - INFO - model.layers.9.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3672, max=0.4258, mean=0.0000, std=0.0297
2025-03-31 23:56:28,134 - INFO - model.layers.9.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.3574, max=0.3926, mean=0.0000, std=0.0369
2025-03-31 23:56:28,136 - INFO - model.layers.9.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1250, max=0.1001, mean=0.0000, std=0.0141
2025-03-31 23:56:28,142 - INFO - model.layers.9.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.4473, max=0.5039, mean=0.0000, std=0.0157
2025-03-31 23:56:28,164 - INFO - model.layers.9.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.5117, max=0.4961, mean=-0.0001, std=0.0217
2025-03-31 23:56:28,187 - INFO - model.layers.9.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.2520, max=0.2471, mean=0.0000, std=0.0171
2025-03-31 23:56:28,208 - INFO - model.layers.9.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.3418, max=0.5664, mean=-0.0000, std=0.0166
2025-03-31 23:56:28,210 - INFO - model.layers.9.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1270, max=1.0391, mean=0.4824, std=0.0616
2025-03-31 23:56:28,210 - INFO - model.layers.9.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0903, max=0.5000, mean=0.3702, std=0.0259
2025-03-31 23:56:28,217 - INFO - model.layers.10.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3184, max=0.3965, mean=-0.0001, std=0.0269
2025-03-31 23:56:28,219 - INFO - model.layers.10.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.2871, max=0.2949, mean=0.0000, std=0.0348
2025-03-31 23:56:28,221 - INFO - model.layers.10.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1680, max=0.1670, mean=0.0000, std=0.0125
2025-03-31 23:56:28,228 - INFO - model.layers.10.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.5156, max=0.6094, mean=-0.0000, std=0.0145
2025-03-31 23:56:28,250 - INFO - model.layers.10.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.5781, max=0.4492, mean=-0.0000, std=0.0216
2025-03-31 23:56:28,272 - INFO - model.layers.10.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.5469, max=0.2930, mean=-0.0000, std=0.0172
2025-03-31 23:56:28,294 - INFO - model.layers.10.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.3887, max=0.5312, mean=0.0000, std=0.0169
2025-03-31 23:56:28,296 - INFO - model.layers.10.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1108, max=1.1172, mean=0.5507, std=0.0723
2025-03-31 23:56:28,297 - INFO - model.layers.10.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0894, max=0.5273, mean=0.4114, std=0.0397
2025-03-31 23:56:28,303 - INFO - model.layers.11.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.2637, max=0.2217, mean=-0.0000, std=0.0273
2025-03-31 23:56:28,306 - INFO - model.layers.11.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.4160, max=0.4375, mean=0.0001, std=0.0377
2025-03-31 23:56:28,307 - INFO - model.layers.11.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1270, max=0.1318, mean=0.0000, std=0.0134
2025-03-31 23:56:28,313 - INFO - model.layers.11.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.6484, max=0.5156, mean=0.0000, std=0.0151
2025-03-31 23:56:28,337 - INFO - model.layers.11.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.4727, max=0.6406, mean=-0.0000, std=0.0219
2025-03-31 23:56:28,358 - INFO - model.layers.11.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.3047, max=0.3203, mean=0.0000, std=0.0176
2025-03-31 23:56:28,380 - INFO - model.layers.11.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.3418, max=0.3848, mean=-0.0000, std=0.0173
2025-03-31 23:56:28,382 - INFO - model.layers.11.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1055, max=0.9922, mean=0.5220, std=0.0589
2025-03-31 23:56:28,382 - INFO - model.layers.11.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1099, max=0.5312, mean=0.4462, std=0.0528
2025-03-31 23:56:28,388 - INFO - model.layers.12.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3770, max=0.3457, mean=-0.0000, std=0.0273
2025-03-31 23:56:28,390 - INFO - model.layers.12.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.7539, max=0.5156, mean=-0.0000, std=0.0362
2025-03-31 23:56:28,392 - INFO - model.layers.12.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1309, max=0.1797, mean=-0.0000, std=0.0144
2025-03-31 23:56:28,398 - INFO - model.layers.12.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.5742, max=0.6992, mean=-0.0000, std=0.0154
2025-03-31 23:56:28,421 - INFO - model.layers.12.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.5156, max=0.6055, mean=-0.0000, std=0.0218
2025-03-31 23:56:28,444 - INFO - model.layers.12.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.2305, max=0.2598, mean=-0.0000, std=0.0180
2025-03-31 23:56:28,465 - INFO - model.layers.12.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.3711, max=0.6289, mean=0.0000, std=0.0177
2025-03-31 23:56:28,467 - INFO - model.layers.12.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0889, max=1.0000, mean=0.4996, std=0.0606
2025-03-31 23:56:28,468 - INFO - model.layers.12.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0967, max=0.5195, mean=0.4679, std=0.0584
2025-03-31 23:56:28,475 - INFO - model.layers.13.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3164, max=0.2930, mean=0.0000, std=0.0265
2025-03-31 23:56:28,477 - INFO - model.layers.13.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.3945, max=0.5352, mean=0.0000, std=0.0334
2025-03-31 23:56:28,479 - INFO - model.layers.13.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.1729, max=0.1699, mean=0.0000, std=0.0171
2025-03-31 23:56:28,485 - INFO - model.layers.13.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.4805, max=0.3848, mean=-0.0000, std=0.0172
2025-03-31 23:56:28,507 - INFO - model.layers.13.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.6211, max=0.6680, mean=-0.0000, std=0.0213
2025-03-31 23:56:28,530 - INFO - model.layers.13.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.4473, max=0.3535, mean=0.0000, std=0.0185
2025-03-31 23:56:28,552 - INFO - model.layers.13.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.4043, max=0.4590, mean=-0.0000, std=0.0181
2025-03-31 23:56:28,554 - INFO - model.layers.13.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=-0.0913, max=1.1875, mean=0.5346, std=0.0850
2025-03-31 23:56:28,554 - INFO - model.layers.13.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.1084, max=0.5312, mean=0.4751, std=0.0530
2025-03-31 23:56:28,561 - INFO - model.layers.14.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.4609, max=0.4258, mean=0.0000, std=0.0268
2025-03-31 23:56:28,563 - INFO - model.layers.14.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.5820, max=0.5234, mean=0.0001, std=0.0305
2025-03-31 23:56:28,566 - INFO - model.layers.14.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.3027, max=0.3027, mean=0.0000, std=0.0227
2025-03-31 23:56:28,572 - INFO - model.layers.14.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3594, max=0.3652, mean=0.0000, std=0.0201
2025-03-31 23:56:28,593 - INFO - model.layers.14.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.8008, max=0.7266, mean=-0.0001, std=0.0221
2025-03-31 23:56:28,616 - INFO - model.layers.14.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.4941, max=0.4102, mean=-0.0000, std=0.0185
2025-03-31 23:56:28,638 - INFO - model.layers.14.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.3828, max=0.4531, mean=0.0000, std=0.0178
2025-03-31 23:56:28,640 - INFO - model.layers.14.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0781, max=1.2500, mean=0.4703, std=0.1198
2025-03-31 23:56:28,640 - INFO - model.layers.14.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0850, max=0.6328, mean=0.5047, std=0.0584
2025-03-31 23:56:28,647 - INFO - model.layers.15.self_attn.q_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.3223, max=0.3340, mean=0.0000, std=0.0249
2025-03-31 23:56:28,649 - INFO - model.layers.15.self_attn.k_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.4902, max=0.4961, mean=0.0001, std=0.0302
2025-03-31 23:56:28,651 - INFO - model.layers.15.self_attn.v_proj.weight: shape=(512, 2048), dtype=torch.float32, min=-0.3008, max=0.2559, mean=-0.0000, std=0.0217
2025-03-31 23:56:28,658 - INFO - model.layers.15.self_attn.o_proj.weight: shape=(2048, 2048), dtype=torch.float32, min=-0.6953, max=0.7344, mean=0.0000, std=0.0197
2025-03-31 23:56:28,679 - INFO - model.layers.15.mlp.gate_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-0.9961, max=0.8555, mean=-0.0000, std=0.0233
2025-03-31 23:56:28,702 - INFO - model.layers.15.mlp.up_proj.weight: shape=(8192, 2048), dtype=torch.float32, min=-1.2344, max=1.0234, mean=-0.0000, std=0.0202
2025-03-31 23:56:28,723 - INFO - model.layers.15.mlp.down_proj.weight: shape=(2048, 8192), dtype=torch.float32, min=-0.8477, max=0.8672, mean=-0.0000, std=0.0178
2025-03-31 23:56:28,725 - INFO - model.layers.15.input_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0835, max=1.1641, mean=0.5031, std=0.1247
2025-03-31 23:56:28,726 - INFO - model.layers.15.post_attention_layernorm.weight: shape=(2048,), dtype=torch.float32, min=0.0576, max=0.6445, mean=0.5089, std=0.0638
2025-03-31 23:56:28,726 - INFO - model.norm.weight: shape=(2048,), dtype=torch.float32, min=0.0306, max=2.9062, mean=2.3526, std=0.2831
2025-03-31 23:56:29,171 - INFO - lm_head.weight: shape=(128256, 2048), dtype=torch.float32, min=-0.3027, max=0.3457, mean=-0.0001, std=0.0219
2025-03-31 23:56:29,171 - INFO - ==========================================

User: Once upon a time

llama3.2-1B: HF Input IDs: [[128000, 12805, 5304, 264, 892]]

HF PostAttnNorm Weight (L0): Mean=0.209254, Std=0.027772, Shape=torch.Size([2048])
  First 5: [0.2041015625, 0.19921875, 0.1845703125, 0.189453125, 0.212890625]
  Last 5: [0.2119140625, 0.2275390625, 0.2138671875, 0.2021484375, 0.2080078125]

[INFO] HF tensors saved to /Users/swap357/Documents/dev/llama3.npy/hf_tensors.npz
HF Layer 0 Output (Last Token): Mean=-0.0002, Std=0.0354, First 5=[-0.009622501209378242, -0.061619460582733154, 0.05686704069375992, -0.027414735406637192, -0.005417127162218094]
HF Final Norm Output (Last Token): Mean=-0.0908, Std=2.3892, First 5=[0.3196721076965332, 4.08328104019165, 2.610888719558716, -0.0061015053652226925, -0.7442800998687744]
HF Initial Top 5 Logits:
  - Token 11 (','): 22.6235
  - Token 1070 (' there'): 21.1501
  - Token 304 (' in'): 20.8061
  - Token 358 (' I'): 19.7913
  - Token 264 (' a'): 19.2808
, there was a man who was very rich. He had a lot of money, and he was very happy with his life. But one day, he decided to take a trip to the city. He was going to visit his friends and see what they were doing. He was also going to see the sights of the city. He was very excited about his trip, and he was looking forward to seeing all the things that he had never seen before.
The man was a very good traveler, and he knew how to take care of himself. He had a lot of money, and he was very careful about how he spent it. He was also very careful about what he ate, and he was very careful about what he drank. He was very
Performance Statistics:
Decode Speed: 13.9 tokens/sec
Time to First Token: 0.3 seconds
Prefill Speed: 15.2 tokens/sec
Model Size: 0 MB
Token Count: 151, Elapsed: 10.86s, 14 tokens/s